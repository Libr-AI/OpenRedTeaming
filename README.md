# Surveys and Collections

https://llmsecurity.net/

https://github.com/chawins/llm-sp

[Awesome LLM-Safety](https://github.com/ydyjya/Awesome-LLM-Safety/tree/main)

[EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://github.com/EasyJailbreak/EasyJailbreak)

[Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/pdf/2310.10844.pdf)

[A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models](https://arxiv.org/pdf/2312.10982.pdf)

[Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements](https://arxiv.org/pdf/2302.09270.pdf)

[Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/pdf/2402.00888.pdf)

# Application Risks

## Agents and GPTs

[Testing Language Model Agents Safely in the Wild](https://arxiv.org/pdf/2311.10538.pdf)

[R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/pdf/2401.10019.pdf)

[Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/pdf/2311.11855.pdf)

[ToolEmu: Identifying the Risks of LM Agents with an LM-Emulated Sandbox](https://arxiv.org/pdf/2309.15817)

[GPT in Sheep's Clothing: The Risk of Customized GPTs](https://arxiv.org/pdf/2401.09075.pdf)

[Assessing Prompt Injection Risks in 200+ Custom GPTs](https://arxiv.org/pdf/2311.11538.pdf)

[PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety](https://arxiv.org/pdf/2401.11880.pdf)


## Fine-tuning

[LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/pdf/2310.20624.pdf)

[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553.pdf)

[Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](https://arxiv.org/pdf/2312.04748.pdf)

[BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/pdf/2312.04127.pdf)

[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/pdf/2310.03693)

[The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/anie.202218122)

[Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions](https://arxiv.org/pdf/2309.07875)

[Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"](https://arxiv.org/pdf/2310.02949)

[Sleeper agents: Training deceptive llms that persist through safety training](https://arxiv.org/pdf/2401.05566.pdf)

[Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)

## Others

[From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?](https://arxiv.org/pdf/2308.01990)

[Transfer Attacks and Defenses for Large Language Models on Coding Tasks](https://arxiv.org/pdf/2311.13445.pdf)

[Unveiling the Dark Side of ChatGPT: Exploring Cyberattacks and Enhancing User Awareness](https://www.preprints.org/manuscript/202309.1768/v1/download)

  

# Multilingual

[Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/pdf/2310.06474.pdf)

[All Languages Matter: On the Multilingual Safety of Large Language Models](https://arxiv.org/pdf/2310.00905)

[Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/pdf/2310.02446.pdf)

[The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://arxiv.org/pdf/2401.13136.pdf)

  

# Risk Taxonomy

[Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks](http://arxiv.org/pdf/2305.14965)

[Can LLMs Follow Simple Rules?](https://arxiv.org/pdf/2311.04235.pdf)

[A Security Risk Taxonomy for Large Language Models](https://arxiv.org/pdf/2311.11415.pdf)

[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)

[On the Risk of Misinformation Pollution with Large Language Models](https://arxiv.org/pdf/2312.10982.pdf)

[Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173)

[Prompt Injection Attacks and Defenses in LLM-Integrated Applications](http://arxiv.org/pdf/2306.04735)

[Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems](http://arxiv.org/pdf/2303.05453)

[Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](http://arxiv.org/pdf/2305.13860)

[Privacy in Large Language Models: Attacks, Defenses and Future Directions](https://arxiv.org/pdf/2310.10383.pdf)

["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/pdf/2308.03825)

[AI Deception: A Survey of Examples, Risks, and Potential Solutions](https://arxiv.org/pdf/2308.14752)

[Safety Assessment of Chinese Large Language Models](https://arxiv.org/pdf/2304.10436.pdf)

[Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://arxiv.org/pdf/2311.16119.pdf)

## Analysis

[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/pdf/2307.02483)

[Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/pdf/2312.06924.pdf)

[Why do universal adversarial attacks work on large language models?: Geometry might be the answer"](https://arxiv.org/pdf/2309.00254)

[Navigating the OverKill in Large Language Models](https://arxiv.org/abs/2401.17633)

[Forbidden Facts: An Investigation of Competing Objectives in Llama-2](https://arxiv.org/pdf/2312.08793.pdf)

  

# Attacking Language Models

## LLM Synthesis

  

[Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks](https://arxiv.org/pdf/2310.12516.pdf)

[Attack Prompt Generation for Red Teaming and Defending Large Language Models](http://arxiv.org/pdf/2306.05499)

[GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/pdf/2309.10253)

[All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks](https://arxiv.org/pdf/2401.09798.pdf)

[An LLM can Fool Itself: A Prompt-Based Adversarial Attack](https://arxiv.org/pdf/2310.13345.pdf)

[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/pdf/2310.08419)

[Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/pdf/2312.02119.pdf)

[Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/pdf/2306.09442)

[TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models](https://arxiv.org/pdf/2306.06815.pdf)

## Template Based

[Goal-Oriented Prompt Attack and Safety Evaluation for LLMs](https://arxiv.org/pdf/2309.11830.pdf) 
[Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/pdf/2306.05499.pdf) 

[Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://arxiv.org/pdf/2311.09827.pdf)

[DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/pdf/2311.03191.pdf)

  
### Composition (2)

[A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/pdf/2311.08268.pdf)

["FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models",](https://arxiv.org/pdf/2309.05274)

[Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks](https://arxiv.org/pdf/2310.10077.pdf)

### Psychology and Persona (3)

[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/pdf/2401.06373.pdf) 

[PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety](https://arxiv.org/pdf/2401.11880.pdf) 

[Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/pdf/2311.03348.pdf)

## Search Based (5)

[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/pdf/2307.15043.pdf) 

[JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models](https://arxiv.org/pdf/2311.00286.pdf) 

["Open Sesame! Universal Black Box Jailbreaking of Large Language Models",](https://arxiv.org/pdf/2309.01446) 

["AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",](https://arxiv.org/pdf/2310.04451) 

[AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/pdf/2310.04451.pdf) 


## Activation Based (4) 

[Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/pdf/2401.06824.pdf) 

[Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/pdf/2311.09433.pdf) 

[Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/pdf/2312.02780.pdf)

[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)

## Decoding Strategy (2) 

[On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?](https://arxiv.org/pdf/2310.01581) 

[Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/pdf/2310.06987.pdf)

## Transfer Attack (3) 

["AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",](https://arxiv.org/pdf/2310.04451)

[Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/pdf/2401.17256.pdf)

[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)


## Other (6) 

[GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"](https://arxiv.org/pdf/2308.06463) 

[Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/pdf/2312.04127.pdf) 

[Bypassing the Safety Training of Open-Source LLMs with Priming Attacks](https://arxiv.org/pdf/2312.12321.pdf)

[MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/pdf/2307.08715.pdf) 

["Multi-step Jailbreaking Privacy Attacks on ChatGPT",](https://arxiv.org/abs/2304.05197) 

[Adversarial Attacks and Defenses in Large Language Models: Old and New Threats](https://arxiv.org/pdf/2310.19737.pdf)

# Attacking Multimodal Models 

[Red Teaming Visual Language Models](https://arxiv.org/pdf/2401.12915.pdf)

## Language Attack

[Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](https://arxiv.org/pdf/2311.09127.pdf) 

[Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/pdf/2312.04127.pdf) 

[Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks](https://arxiv.org/pdf/2312.14440.pdf)

[FLIRT: Feedback Loop In-context Red Teaming](https://arxiv.org/pdf/2308.04265.pdf)

## Vision Attack

[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/pdf/2311.05608.pdf) 

[How Robust is Google's Bard to Adversarial Image Attacks?"](https://arxiv.org/pdf/2309.11751) 

[How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/pdf/2311.16101.pdf) 

[Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://arxiv.org/pdf/2307.14539.pdf) 

[Query-Relevant Images Jailbreak Large Multi-Modal Models](https://arxiv.org/pdf/2311.17600.pdf) 

[On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](https://arxiv.org/pdf/2312.03777.pdf)

[Black box adversarial prompting for foundation models](https://arxiv.org/pdf/2302.04237.pdf)

## Intermodal Attack

[SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation](https://arxiv.org/pdf/2312.04913.pdf) 

[MMA-Diffusion: MultiModal Attack on Diffusion Models](https://arxiv.org/pdf/2311.17516.pdf) 

[InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/pdf/2312.01886.pdf)


# Defense

## Prompting

[Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/pdf/2401.06561.pdf) 

[Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/pdf/2310.06387) 

[Self-Guard: Empower the LLM to Safeguard Itself](https://arxiv.org/pdf/2310.15851.pdf) 

[Using In-Context Learning to Improve Dialogue Safety](https://arxiv.org/pdf/2302.00871.pdf) 

[Defending ChatGPT against jailbreak attack via self-reminders](https://www.nature.com/articles/s42256-023-00765-8)

## RLHF

[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/pdf/2307.04657) 

[The History and Risks of Reinforcement Learning and Human Feedback](https://arxiv.org/pdf/2310.13595.pdf) 

[Safer-Instruct: Aligning Language Models with Automated Preference Data](https://arxiv.org/pdf/2311.08685.pdf) 

[Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2310.12773.pdf)

[Mart: Improving llm safety with multi-round automatic red-teaming](https://arxiv.org/pdf/2311.07689.pdf)

## Multi-Stage

["NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails"](https://arxiv.org/pdf/2310.10501.pdf) 

[GUARDIAN: A Multi-Tiered Defense Architecture for Thwarting Prompt Injection Attacks on LLMs](https://www.scirp.org/journal/paperinformation?paperid=130663) 

[Baseline Defenses for Adversarial Attacks Against Aligned Language Models](https://arxiv.org/pdf/2309.00614.pdf)

[Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/pdf/2312.00029.pdf)

## Language Models Ensemble

[Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/pdf/2310.02417.pdf)

[Combating Adversarial Attacks with Multi-Agent Debate](https://arxiv.org/pdf/2401.05998.pdf)

## Fine-tuning

[Self-Guard: Empower the LLM to Safeguard Itself](https://arxiv.org/pdf/2310.15851.pdf) 

[Recipes for safety in open-domain chatbots](https://arxiv.org/pdf/2010.07079.pdf) 

[Learn What NOT to Learn: Towards Generative Safety in Chatbots](https://arxiv.org/pdf/2304.11220.pdf)

[Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/pdf/2401.10862.pdf)

## Targets

### Against Adversarial Suffixes

[Detecting Language Model Attacks with Perplexity](https://arxiv.org/pdf/2308.14132.pdf)

[Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/pdf/2309.02705.pdf)

[SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/pdf/2310.03684)

### Multimodal Defense

[MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](https://arxiv.org/pdf/2401.02906.pdf)

  

# Evaluation

## Metrics

[A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models](https://arxiv.org/pdf/2401.00991.pdf)

[AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/pdf/2401.09002.pdf)

[Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success](https://arxiv.org/pdf/2307.06865)

[Beyond the Safeguards: Exploring the Security Risks of ChatGPT](http://arxiv.org/pdf/2305.08005)

["It's a Fair Game or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"](https://arxiv.org/pdf/2309.11653)



## Benchmarks 

[Walking a Tightrope \u9225?Evaluating Large Language Models in High-Risk Domains](https://arxiv.org/pdf/2311.14966.pdf)

[SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese](https://arxiv.org/pdf/2310.05818)

[DICES Dataset: Diversity in Conversational AI Evaluation for Safety](http://arxiv.org/pdf/2306.11247)

[Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/pdf/2312.14197.pdf)

[Control Risk for Potential Misuse of Artificial Intelligence in Science](https://arxiv.org/pdf/2312.06632.pdf) 

[SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/pdf/2311.08370.pdf) 

[Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org/pdf/2311.01011.pdf) 

[SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions](https://arxiv.org/pdf/2309.07045) 

[Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/pdf/2308.13387.pdf) 

[CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility](https://arxiv.org/pdf/2307.09705) 

[XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/pdf/2308.01263) 

["Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models",](https://arxiv.org/pdf/2307.08487)

[Tall tales at different scales: Evaluating scaling trends for deception in language models](https://openreview.net/forum?id=YRXDl6I3j5)

[How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation](https://arxiv.org/pdf/2312.17115.pdf)
