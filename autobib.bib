@article{2023Erfanarxiv:2310.10844v1,
 author = {Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
 journal = {arxiv:2310.10844v1},
 title = {Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks},
 url = {http://arxiv.org/abs/2310.10844v1},
 year = {2023}
}

@article{2023Aysanarxiv:2312.10982v1,
 author = {Aysan Esmradi and Daniel Wankit Yip and Chun Fai Chan},
 journal = {arxiv:2312.10982v1},
 title = {A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models},
 url = {http://arxiv.org/abs/2312.10982v1},
 year = {2023}
}

@article{2023Jiawenarxiv:2302.09270v3,
 author = {Jiawen Deng and Jiale Cheng and Hao Sun and Zhexin Zhang and Minlie Huang},
 journal = {arxiv:2302.09270v3},
 title = {Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements},
 url = {http://arxiv.org/abs/2302.09270v3},
 year = {2023}
}

@article{2023Silenarxiv:2311.10538v3,
 author = {Silen Naihin and David Atkinson and Marc Green and Merwane Hamadi and Craig Swift and Douglas Schonholtz and Adam Tauman Kalai and David Bau},
 journal = {arxiv:2311.10538v3},
 title = {Testing Language Model Agents Safely in the Wild},
 url = {http://arxiv.org/abs/2311.10538v3},
 year = {2023}
}

@misc{yuan2024rjudge,
      title={R-Judge: Benchmarking Safety Risk Awareness for LLM Agents}, 
      author={Tongxin Yuan and Zhiwei He and Lingzhong Dong and Yiming Wang and Ruijie Zhao and Tian Xia and Lizhen Xu and Binglin Zhou and Fangqi Li and Zhuosheng Zhang and Rui Wang and Gongshen Liu},
      year={2024},
      eprint={2401.10019},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Yuarxiv:2311.11855v1,
 author = {Yu Tian and Xiao Yang and Jingyuan Zhang and Yinpeng Dong and Hang Su},
 journal = {arxiv:2311.11855v1},
 title = {Evil Geniuses: Delving into the Safety of LLM-based Agents},
 url = {http://arxiv.org/abs/2311.11855v1},
 year = {2023}
}

@misc{ruan2023identifying,
      title={Identifying the Risks of LM Agents with an LM-Emulated Sandbox}, 
      author={Yangjun Ruan and Honghua Dong and Andrew Wang and Silviu Pitis and Yongchao Zhou and Jimmy Ba and Yann Dubois and Chris J. Maddison and Tatsunori Hashimoto},
      year={2023},
      eprint={2309.15817},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{2024Sagivarxiv:2401.09075v1,
 author = {Sagiv Antebi and Noam Azulay and Edan Habler and Ben Ganon and Asaf Shabtai and Yuval Elovici},
 journal = {arxiv:2401.09075v1},
 title = {GPT in Sheep's Clothing: The Risk of Customized GPTs},
 url = {http://arxiv.org/abs/2401.09075v1},
 year = {2024}
}

@article{2023Jiahaoarxiv:2311.11538v1,
 author = {Jiahao Yu and Yuhang Wu and Dong Shu and Mingyu Jin and Xinyu Xing},
 journal = {arxiv:2311.11538v1},
 title = {Assessing Prompt Injection Risks in 200+ Custom GPTs},
 url = {http://arxiv.org/abs/2311.11538v1},
 year = {2023}
}

@misc{zhang2024psysafe,
      title={PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety}, 
      author={Zaibin Zhang and Yongting Zhang and Lijun Li and Hongzhi Gao and Lijun Wang and Huchuan Lu and Feng Zhao and Yu Qiao and Jing Shao},
      year={2024},
      eprint={2401.11880},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Simonarxiv:2310.20624v1,
 author = {Simon Lermen and Charlie Rogers-Smith and Jeffrey Ladish},
 journal = {arxiv:2310.20624v1},
 title = {LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B},
 url = {http://arxiv.org/abs/2310.20624v1},
 year = {2023}
}

@article{2023Qiusiarxiv:2311.05553v2,
 author = {Qiusi Zhan and Richard Fang and Rohan Bindu and Akul Gupta and Tatsunori Hashimoto and Daniel Kang},
 journal = {arxiv:2311.05553v2},
 title = {Removing RLHF Protections in GPT-4 via Fine-Tuning},
 url = {http://arxiv.org/abs/2311.05553v2},
 year = {2023}
}

@article{2023Shuliarxiv:2312.04748v1,
 author = {Shuli Jiang and Swanand Ravindra Kadhe and Yi Zhou and Ling Cai and Nathalie Baracaldo},
 journal = {arxiv:2312.04748v1},
 title = {Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks},
 url = {http://arxiv.org/abs/2312.04748v1},
 year = {2023}
}

@misc{gade2023badllama,
      title={BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B}, 
      author={Pranav Gade and Simon Lermen and Charlie Rogers-Smith and Jeffrey Ladish},
      year={2023},
      eprint={2311.00117},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qi2023finetuning,
      title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!}, 
      author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
      year={2023},
      eprint={2310.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Xiaoyiarxiv:2310.15469v1,
 author = {Xiaoyi Chen and Siyuan Tang and Rui Zhu and Shijun Yan and Lei Jin and Zihao Wang and Liya Su and XiaoFeng Wang and Haixu Tang},
 journal = {arxiv:2310.15469v1},
 title = {The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks},
 url = {http://arxiv.org/abs/2310.15469v1},
 year = {2023}
}

@article{2023Federicoarxiv:2309.07875v2,
 author = {Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Röttger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
 journal = {arxiv:2309.07875v2},
 title = {Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
 url = {http://arxiv.org/abs/2309.07875v2},
 year = {2023}
}

@misc{yang2023shadow,
      title={Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models}, 
      author={Xianjun Yang and Xiao Wang and Qi Zhang and Linda Petzold and William Yang Wang and Xun Zhao and Dahua Lin},
      year={2023},
      eprint={2310.02949},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2024Evanarxiv:2401.05566v3,
 author = {Evan Hubinger and Carson Denison and Jesse Mu and Mike Lambert and Meg Tong and Monte MacDiarmid and Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and Newton Cheng and Adam Jermyn and Amanda Askell and Ansh Radhakrishnan and Cem Anil and David Duvenaud and Deep Ganguli and Fazl Barez and Jack Clark and Kamal Ndousse and Kshitij Sachan and Michael Sellitto and Mrinank Sharma and Nova DasSarma and Roger Grosse and Shauna Kravec and Yuntao Bai and Zachary Witten and Marina Favaro and Jan Brauner and Holden Karnofsky and Paul Christiano and Samuel R. Bowman and Logan Graham and Jared Kaplan and Sören Mindermann and Ryan Greenblatt and Buck Shlegeris and Nicholas Schiefer and Ethan Perez},
 journal = {arxiv:2401.05566v3},
 title = {Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training},
 url = {http://arxiv.org/abs/2401.05566v3},
 year = {2024}
}

@article{2023Vaidehiarxiv:2309.17410v1,
 author = {Vaidehi Patil and Peter Hase and Mohit Bansal},
 journal = {arxiv:2309.17410v1},
 title = {Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks},
 url = {http://arxiv.org/abs/2309.17410v1},
 year = {2023}
}

@article{2023Rodrigoarxiv:2308.01990v3,
 author = {Rodrigo Pedro and Daniel Castro and Paulo Carreira and Nuno Santos},
 journal = {arxiv:2308.01990v3},
 title = {From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?},
 url = {http://arxiv.org/abs/2308.01990v3},
 year = {2023}
}

@article{2023Chiarxiv:2311.13445v1,
 author = {Chi Zhang and Zifan Wang and Ravi Mangal and Matt Fredrikson and Limin Jia and Corina Pasareanu},
 journal = {arxiv:2311.13445v1},
 title = {Transfer Attacks and Defenses for Large Language Models on Coding Tasks},
 url = {http://arxiv.org/abs/2311.13445v1},
 year = {2023}
}

@article{2023Zhouboarxiv:2310.02129v2,
 author = {Zhoubo Li and Ningyu Zhang and Yunzhi Yao and Mengru Wang and Xi Chen and Huajun Chen},
 journal = {arxiv:2310.02129v2},
 title = {Unveiling the Pitfalls of Knowledge Editing for Large Language Models},
 url = {http://arxiv.org/abs/2310.02129v2},
 year = {2023}
}

@article{2023Yuearxiv:2310.06474v1,
 author = {Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing},
 journal = {arxiv:2310.06474v1},
 title = {Multilingual Jailbreak Challenges in Large Language Models},
 url = {http://arxiv.org/abs/2310.06474v1},
 year = {2023}
}

@misc{deng2023multilingual,
      title={Multilingual Jailbreak Challenges in Large Language Models}, 
      author={Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing},
      year={2023},
      eprint={2310.06474},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Wenxuanarxiv:2310.00905v1,
 author = {Wenxuan Wang and Zhaopeng Tu and Chang Chen and Youliang Yuan and Jen-tse Huang and Wenxiang Jiao and Michael R. Lyu},
 journal = {arxiv:2310.00905v1},
 title = {All Languages Matter: On the Multilingual Safety of Large Language Models},
 url = {http://arxiv.org/abs/2310.00905v1},
 year = {2023}
}

@article{2023Zheng-Xinarxiv:2310.02446v2,
 author = {Zheng-Xin Yong and Cristina Menghini and Stephen H. Bach},
 journal = {arxiv:2310.02446v2},
 title = {Low-Resource Languages Jailbreak GPT-4},
 url = {http://arxiv.org/abs/2310.02446v2},
 year = {2023}
}

@article{2024Lingfengarxiv:2401.13136v1,
 author = {Lingfeng Shen and Weiting Tan and Sihao Chen and Yunmo Chen and Jingyu Zhang and Haoran Xu and Boyuan Zheng and Philipp Koehn and Daniel Khashabi},
 journal = {arxiv:2401.13136v1},
 title = {The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts},
 url = {http://arxiv.org/abs/2401.13136v1},
 year = {2024}
}

@article{2023Abhinavarxiv:2305.14965v1,
 author = {Abhinav Rao and Sachin Vashistha and Atharva Naik and Somak Aditya and Monojit Choudhury},
 journal = {arxiv:2305.14965v1},
 title = {Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks},
 url = {http://arxiv.org/abs/2305.14965v1},
 year = {2023}
}

@article{2023Normanarxiv:2311.04235v1,
 author = {Norman Mu and Sarah Chen and Zifan Wang and Sizhe Chen and David Karamardian and Lulwa Aljeraisy and Dan Hendrycks and David Wagner},
 journal = {arxiv:2311.04235v1},
 title = {Can LLMs Follow Simple Rules?},
 url = {http://arxiv.org/abs/2311.04235v1},
 year = {2023}
}

@article{2023Erikarxiv:2311.11415v1,
 author = {Erik Derner and Kristina Batistič and Jan Zahálka and Robert Babuška},
 journal = {arxiv:2311.11415v1},
 title = {A Security Risk Taxonomy for Large Language Models},
 url = {http://arxiv.org/abs/2311.11415v1},
 year = {2023}
}

@article{2023Nannaarxiv:2311.06237v2,
 author = {Nanna Inie and Jonathan Stray and Leon Derczynski},
 journal = {arxiv:2311.06237v2},
 title = {Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild},
 url = {http://arxiv.org/abs/2311.06237v2},
 year = {2023}
}

@article{2023Yikangarxiv:2305.13661v2,
 author = {Yikang Pan and Liangming Pan and Wenhu Chen and Preslav Nakov and Min-Yen Kan and William Yang Wang},
 journal = {arxiv:2305.13661v2},
 title = {On the Risk of Misinformation Pollution with Large Language Models},
 url = {http://arxiv.org/abs/2305.13661v2},
 year = {2023}
}

@article{2023Kaiarxiv:2302.12173v2,
 author = {Kai Greshake and Sahar Abdelnabi and Shailesh Mishra and Christoph Endres and Thorsten Holz and Mario Fritz},
 journal = {arxiv:2302.12173v2},
 title = {Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
 url = {http://arxiv.org/abs/2302.12173v2},
 year = {2023}
}

@article{2023Yupeiarxiv:2310.12815v1,
 author = {Yupei Liu and Yuqi Jia and Runpeng Geng and Jinyuan Jia and Neil Zhenqiang Gong},
 journal = {arxiv:2310.12815v1},
 title = {Prompt Injection Attacks and Defenses in LLM-Integrated Applications},
 url = {http://arxiv.org/abs/2310.12815v1},
 year = {2023}
}

@article{2024Tianyuarxiv:2401.05778v1,
 author = {Tianyu Cui and Yanling Wang and Chuanpu Fu and Yong Xiao and Sijia Li and Xinhao Deng and Yunpeng Liu and Qinglin Zhang and Ziyi Qiu and Peiyang Li and Zhixing Tan and Junwu Xiong and Xinyu Kong and Zujie Wen and Ke Xu and Qi Li},
 journal = {arxiv:2401.05778v1},
 title = {Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems},
 url = {http://arxiv.org/abs/2401.05778v1},
 year = {2024}
}

@article{2023Yiarxiv:2305.13860v1,
 author = {Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
 journal = {arxiv:2305.13860v1},
 title = {Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
 url = {http://arxiv.org/abs/2305.13860v1},
 year = {2023}
}

@article{2023Haoranarxiv:2310.10383v1,
 author = {Haoran Li and Yulin Chen and Jinglong Luo and Yan Kang and Xiaojin Zhang and Qi Hu and Chunkit Chan and Yangqiu Song},
 journal = {arxiv:2310.10383v1},
 title = {Privacy in Large Language Models: Attacks, Defenses and Future Directions},
 url = {http://arxiv.org/abs/2310.10383v1},
 year = {2023}
}

@misc{shen2023do,
      title={"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models}, 
      author={Xinyue Shen and Zeyuan Chen and Michael Backes and Yun Shen and Yang Zhang},
      year={2023},
      eprint={2308.03825},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{2023Peterarxiv:2308.14752v1,
 author = {Peter S. Park and Simon Goldstein and Aidan O'Gara and Michael Chen and Dan Hendrycks},
 journal = {arxiv:2308.14752v1},
 title = {AI Deception: A Survey of Examples, Risks, and Potential Solutions},
 url = {http://arxiv.org/abs/2308.14752v1},
 year = {2023}
}

@article{2023Haoarxiv:2304.10436v1,
 author = {Hao Sun and Zhexin Zhang and Jiawen Deng and Jiale Cheng and Minlie Huang},
 journal = {arxiv:2304.10436v1},
 title = {Safety Assessment of Chinese Large Language Models},
 url = {http://arxiv.org/abs/2304.10436v1},
 year = {2023}
}

@article{2023Sanderarxiv:2311.16119v2,
 author = {Sander Schulhoff and Jeremy Pinto and Anaum Khan and Louis-François Bouchard and Chenglei Si and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher Carnahan and Jordan Boyd-Graber},
 journal = {arxiv:2311.16119v2},
 title = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition},
 url = {http://arxiv.org/abs/2311.16119v2},
 year = {2023}
}

@misc{wei2023jailbroken,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2023Yuarxiv:2312.06924v1,
 author = {Yu Fu and Yufei Li and Wen Xiao and Cong Liu and Yue Dong},
 journal = {arxiv:2312.06924v1},
 title = {Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack},
 url = {http://arxiv.org/abs/2312.06924v1},
 year = {2023}
}

@article{2024Chenyuarxiv:2401.17633v1,
 author = {Chenyu Shi and Xiao Wang and Qiming Ge and Songyang Gao and Xianjun Yang and Tao Gui and Qi Zhang and Xuanjing Huang and Xun Zhao and Dahua Lin},
 journal = {arxiv:2401.17633v1},
 title = {Navigating the OverKill in Large Language Models},
 url = {http://arxiv.org/abs/2401.17633v1},
 year = {2024}
}

@article{2023Tonyarxiv:2312.08793v3,
 author = {Tony T. Wang and Miles Wang and Kaivalya Hariharan and Nir Shavit},
 journal = {arxiv:2312.08793v3},
 title = {Forbidden Facts: An Investigation of Competing Objectives in Llama-2},
 url = {http://arxiv.org/abs/2312.08793v3},
 year = {2023}
}

@article{2023Xiaodongarxiv:2310.12516v1,
 author = {Xiaodong Yu and Hao Cheng and Xiaodong Liu and Dan Roth and Jianfeng Gao},
 journal = {arxiv:2310.12516v1},
 title = {Automatic Hallucination Assessment for Aligned Large Language Models via
  Transferable Adversarial Attacks},
 url = {http://arxiv.org/abs/2310.12516v1},
 year = {2023}
}

@article{2023Boyiarxiv:2310.12505v1,
 author = {Boyi Deng and Wenjie Wang and Fuli Feng and Yang Deng and Qifan Wang and Xiangnan He},
 journal = {arxiv:2310.12505v1},
 title = {Attack Prompt Generation for Red Teaming and Defending Large Language
  Models},
 url = {http://arxiv.org/abs/2310.12505v1},
 year = {2023}
}

@misc{yu2023gptfuzzer,
      title={GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts}, 
      author={Jiahao Yu and Xingwei Lin and Zheng Yu and Xinyu Xing},
      year={2023},
      eprint={2309.10253},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{2024Kazuhiroarxiv:2401.09798v2,
 author = {Kazuhiro Takemoto},
 journal = {arxiv:2401.09798v2},
 title = {All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks},
 url = {http://arxiv.org/abs/2401.09798v2},
 year = {2024}
}

@article{2023Xiliearxiv:2310.13345v1,
 author = {Xilie Xu and Keyi Kong and Ning Liu and Lizhen Cui and Di Wang and Jingfeng Zhang and Mohan Kankanhalli},
 journal = {arxiv:2310.13345v1},
 title = {An LLM can Fool Itself: A Prompt-Based Adversarial Attack},
 url = {http://arxiv.org/abs/2310.13345v1},
 year = {2023}
}

@article{2023Patrickarxiv:2310.08419v2,
 author = {Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
 journal = {arxiv:2310.08419v2},
 title = {Jailbreaking Black Box Large Language Models in Twenty Queries},
 url = {http://arxiv.org/abs/2310.08419v2},
 year = {2023}
}

@article{2023Anayarxiv:2312.02119v1,
 author = {Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi},
 journal = {arxiv:2312.02119v1},
 title = {Tree of Attacks: Jailbreaking Black-Box LLMs Automatically},
 url = {http://arxiv.org/abs/2312.02119v1},
 year = {2023}
}

@misc{bhardwaj2023redteaming,
      title={Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment}, 
      author={Rishabh Bhardwaj and Soujanya Poria},
      year={2023},
      eprint={2308.09662},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{xue2023trojllm,
    title={Troj{LLM}: A Black-box Trojan Prompt Attack on Large Language Models},
    author={Jiaqi Xue and Mengxin Zheng and Ting Hua and Yilin Shen and Yepeng Liu and Ladislau B{\"o}l{\"o}ni and Qian Lou},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=ZejTutd7VY}
}


@article{2023Chengyuanarxiv:2309.11830v2,
 author = {Chengyuan Liu and Fubang Zhao and Lizhi Qing and Yangyang Kang and Changlong Sun and Kun Kuang and Fei Wu},
 journal = {arxiv:2309.11830v2},
 title = {Goal-Oriented Prompt Attack and Safety Evaluation for LLMs},
 url = {http://arxiv.org/abs/2309.11830v2},
 year = {2023}
}

@article{2023Yiarxiv:2306.05499v1,
 author = {Yi Liu and Gelei Deng and Yuekang Li and Kailong Wang and Tianwei Zhang and Yepang Liu and Haoyu Wang and Yan Zheng and Yang Liu},
 journal = {arxiv:2306.05499v1},
 title = {Prompt Injection attack against LLM-integrated Applications},
 url = {http://arxiv.org/abs/2306.05499v1},
 year = {2023}
}

@article{2023Nanarxiv:2311.09827v1,
 author = {Nan Xu and Fei Wang and Ben Zhou and Bang Zheng Li and Chaowei Xiao and Muhao Chen},
 journal = {arxiv:2311.09827v1},
 title = {Cognitive Overload: Jailbreaking Large Language Models with Overloaded
  Logical Thinking},
 url = {http://arxiv.org/abs/2311.09827v1},
 year = {2023}
}

@misc{li2023deepinception,
      title={DeepInception: Hypnotize Large Language Model to Be Jailbreaker}, 
      author={Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han},
      year={2023},
      eprint={2311.03191},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2023Pengarxiv:2311.08268v1,
 author = {Peng Ding and Jun Kuang and Dan Ma and Xuezhi Cao and Yunsen Xian and Jiajun Chen and Shujian Huang},
 journal = {arxiv:2311.08268v1},
 title = {A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can
  Fool Large Language Models Easily},
 url = {http://arxiv.org/abs/2311.08268v1},
 year = {2023}
}

@misc{yao2023fuzzllm,
      title={FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models}, 
      author={Dongyu Yao and Jianshu Zhang and Ian G. Harris and Marcel Carlsson},
      year={2023},
      eprint={2309.05274},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{2023Shuyuarxiv:2310.10077v1,
 author = {Shuyu Jiang and Xingshu Chen and Rui Tang},
 journal = {arxiv:2310.10077v1},
 title = {Prompt Packer: Deceiving LLMs through Compositional Instruction with
  Hidden Attacks},
 url = {http://arxiv.org/abs/2310.10077v1},
 year = {2023}
}

@article{2024Yiarxiv:2401.06373v2,
 author = {Yi Zeng and Hongpeng Lin and Jingwen Zhang and Diyi Yang and Ruoxi Jia and Weiyan Shi},
 journal = {arxiv:2401.06373v2},
 title = {How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to
  Challenge AI Safety by Humanizing LLMs},
 url = {http://arxiv.org/abs/2401.06373v2},
 year = {2024}
}

@article{2023Rushebarxiv:2311.03348v2,
 author = {Rusheb Shah and Quentin Feuillade--Montixi and Soroush Pour and Arush Tagade and Stephen Casper and Javier Rando},
 journal = {arxiv:2311.03348v2},
 title = {Scalable and Transferable Black-Box Jailbreaks for Language Models via
  Persona Modulation},
 url = {http://arxiv.org/abs/2311.03348v2},
 year = {2023}
}

@article{2023Andyarxiv:2307.15043v2,
 author = {Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
 journal = {arxiv:2307.15043v2},
 title = {Universal and Transferable Adversarial Attacks on Aligned Language
  Models},
 url = {http://arxiv.org/abs/2307.15043v2},
 year = {2023}
}

@misc{zhang2023jade,
      title={JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models}, 
      author={Mi Zhang and Xudong Pan and Min Yang},
      year={2023},
      eprint={2311.00286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lapid2023open,
      title={Open Sesame! Universal Black Box Jailbreaking of Large Language Models}, 
      author={Raz Lapid and Ron Langberg and Moshe Sipper},
      year={2023},
      eprint={2309.01446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2023autodan,
      title={AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models}, 
      author={Sicheng Zhu and Ruiyi Zhang and Bang An and Gang Wu and Joe Barrow and Zichao Wang and Furong Huang and Ani Nenkova and Tong Sun},
      year={2023},
      eprint={2310.15140},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{liu2023autodan,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}, 
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      year={2023},
      eprint={2310.04451},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2024Tianlongarxiv:2401.06824v1,
 author = {Tianlong Li and Xiaoqing Zheng and Xuanjing Huang},
 journal = {arxiv:2401.06824v1},
 title = {Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation
  Engineering},
 url = {http://arxiv.org/abs/2401.06824v1},
 year = {2024}
}

@article{2023Haoranarxiv:2311.09433v2,
 author = {Haoran Wang and Kai Shu},
 journal = {arxiv:2311.09433v2},
 title = {Backdoor Activation Attack: Attack Large Language Models using
  Activation Steering for Safety-Alignment},
 url = {http://arxiv.org/abs/2311.09433v2},
 year = {2023}
}

@article{2023Stanislavarxiv:2312.02780v1,
 author = {Stanislav Fort},
 journal = {arxiv:2312.02780v1},
 title = {Scaling Laws for Adversarial Attacks on Language Model Activations},
 url = {http://arxiv.org/abs/2312.02780v1},
 year = {2023}
}

@article{2023Hangfanarxiv:2310.01581v1,
 author = {Hangfan Zhang and Zhimeng Guo and Huaisheng Zhu and Bochuan Cao and Lu Lin and Jinyuan Jia and Jinghui Chen and Dinghao Wu},
 journal = {arxiv:2310.01581v1},
 title = {On the Safety of Open-Sourced Large Language Models: Does Alignment
  Really Prevent Them From Being Misused?},
 url = {http://arxiv.org/abs/2310.01581v1},
 year = {2023}
}

@article{2023Yangsiboarxiv:2310.06987v1,
 author = {Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen},
 journal = {arxiv:2310.06987v1},
 title = {Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation},
 url = {http://arxiv.org/abs/2310.06987v1},
 year = {2023}
}

@article{2024Xuandongarxiv:2401.17256v1,
 author = {Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
 journal = {arxiv:2401.17256v1},
 title = {Weak-to-Strong Jailbreaking on Large Language Models},
 url = {http://arxiv.org/abs/2401.17256v1},
 year = {2024}
}

@misc{zhao2024weaktostrong,
      title={Weak-to-Strong Jailbreaking on Large Language Models}, 
      author={Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
      year={2024},
      eprint={2401.17256},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yuan2023gpt4,
      title={GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher}, 
      author={Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
      year={2023},
      eprint={2308.06463},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Yanruiarxiv:2312.04127v1,
 author = {Yanrui Du and Sendong Zhao and Ming Ma and Yuhan Chen and Bing Qin},
 journal = {arxiv:2312.04127v1},
 title = {Analyzing the Inherent Response Tendency of LLMs: Real-World
  Instructions-Driven Jailbreak},
 url = {http://arxiv.org/abs/2312.04127v1},
 year = {2023}
}

@article{2023Jasonarxiv:2312.12321v1,
 author = {Jason Vega and Isha Chaudhary and Changming Xu and Gagandeep Singh},
 journal = {arxiv:2312.12321v1},
 title = {Bypassing the Safety Training of Open-Source LLMs with Priming Attacks},
 url = {http://arxiv.org/abs/2312.12321v1},
 year = {2023}
}

@article{2023Haoranarxiv:2304.05197v3,
 author = {Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Fanpu Meng and Yangqiu Song},
 journal = {arxiv:2304.05197v3},
 title = {Multi-step Jailbreaking Privacy Attacks on ChatGPT},
 url = {http://arxiv.org/abs/2304.05197v3},
 year = {2023}
}

@misc{deng2023masterkey,
      title={MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots}, 
      author={Gelei Deng and Yi Liu and Yuekang Li and Kailong Wang and Ying Zhang and Zefeng Li and Haoyu Wang and Tianwei Zhang and Yang Liu},
      year={2023},
      eprint={2307.08715},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{2023Leoarxiv:2310.19737v1,
 author = {Leo Schwinn and David Dobre and Stephan Günnemann and Gauthier Gidel},
 journal = {arxiv:2310.19737v1},
 title = {Adversarial Attacks and Defenses in Large Language Models: Old and New
  Threats},
 url = {http://arxiv.org/abs/2310.19737v1},
 year = {2023}
}

@article{2024Mukaiarxiv:2401.12915v1,
 author = {Mukai Li and Lei Li and Yuwei Yin and Masood Ahmed and Zhenguang Liu and Qi Liu},
 journal = {arxiv:2401.12915v1},
 title = {Red Teaming Visual Language Models},
 url = {http://arxiv.org/abs/2401.12915v1},
 year = {2024}
}

@article{2023Yuanweiarxiv:2311.09127v2,
 author = {Yuanwei Wu and Xiang Li and Yixin Liu and Pan Zhou and Lichao Sun},
 journal = {arxiv:2311.09127v2},
 title = {Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts},
 url = {http://arxiv.org/abs/2311.09127v2},
 year = {2023}
}

@article{2023Xiangyuarxiv:2306.13213v2,
 author = {Xiangyu Qi and Kaixuan Huang and Ashwinee Panda and Peter Henderson and Mengdi Wang and Prateek Mittal},
 journal = {arxiv:2306.13213v2},
 title = {Visual Adversarial Examples Jailbreak Aligned Large Language Models},
 url = {http://arxiv.org/abs/2306.13213v2},
 year = {2023}
}

@article{2023Hazarxiv:2312.14440v1,
 author = {Haz Sameen Shahgir and Xianghao Kong and Greg Ver Steeg and Yue Dong},
 journal = {arxiv:2312.14440v1},
 title = {Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks},
 url = {http://arxiv.org/abs/2312.14440v1},
 year = {2023}
}

@misc{mehrabi2023flirt,
      title={FLIRT: Feedback Loop In-context Red Teaming}, 
      author={Ninareh Mehrabi and Palash Goyal and Christophe Dupuy and Qian Hu and Shalini Ghosh and Richard Zemel and Kai-Wei Chang and Aram Galstyan and Rahul Gupta},
      year={2023},
      eprint={2308.04265},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{gong2023figstep,
      title={FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts}, 
      author={Yichen Gong and Delong Ran and Jinyuan Liu and Conglei Wang and Tianshuo Cong and Anyu Wang and Sisi Duan and Xiaoyun Wang},
      year={2023},
      eprint={2311.05608},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{dong2023robust,
      title={How Robust is Google's Bard to Adversarial Image Attacks?}, 
      author={Yinpeng Dong and Huanran Chen and Jiawei Chen and Zhengwei Fang and Xiao Yang and Yichi Zhang and Yu Tian and Hang Su and Jun Zhu},
      year={2023},
      eprint={2309.11751},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{2023Haoqinarxiv:2311.16101v1,
 author = {Haoqin Tu and Chenhang Cui and Zijun Wang and Yiyang Zhou and Bingchen Zhao and Junlin Han and Wangchunshu Zhou and Huaxiu Yao and Cihang Xie},
 journal = {arxiv:2311.16101v1},
 title = {How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for
  Vision LLMs},
 url = {http://arxiv.org/abs/2311.16101v1},
 year = {2023}
}

@misc{shayegani2023jailbreak,
      title={Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models}, 
      author={Erfan Shayegani and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2307.14539},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{liu2023queryrelevant,
      title={Query-Relevant Images Jailbreak Large Multi-Modal Models}, 
      author={Xin Liu and Yichen Zhu and Yunshi Lan and Chao Yang and Yu Qiao},
      year={2023},
      eprint={2311.17600},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{2023Xuanmingarxiv:2312.03777v2,
 author = {Xuanming Cui and Alejandro Aparcedo and Young Kyun Jang and Ser-Nam Lim},
 journal = {arxiv:2312.03777v2},
 title = {On the Robustness of Large Multimodal Models Against Image Adversarial
  Attacks},
 url = {http://arxiv.org/abs/2312.03777v2},
 year = {2023}
}

@article{2023Nataliearxiv:2302.04237v2,
 author = {Natalie Maus and Patrick Chao and Eric Wong and Jacob Gardner},
 journal = {arxiv:2302.04237v2},
 title = {Black Box Adversarial Prompting for Foundation Models},
 url = {http://arxiv.org/abs/2302.04237v2},
 year = {2023}
}

@misc{he2023saattack,
      title={SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation}, 
      author={Bangyan He and Xiaojun Jia and Siyuan Liang and Tianrui Lou and Yang Liu and Xiaochun Cao},
      year={2023},
      eprint={2312.04913},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yang2023mmadiffusion,
      title={MMA-Diffusion: MultiModal Attack on Diffusion Models}, 
      author={Yijun Yang and Ruiyuan Gao and Xiaosen Wang and Tsung-Yi Ho and Nan Xu and Qiang Xu},
      year={2023},
      eprint={2311.17516},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{wang2024instructta,
      title={InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models}, 
      author={Xunguang Wang and Zhenlan Ji and Pingchuan Ma and Zongjie Li and Shuai Wang},
      year={2024},
      eprint={2312.01886},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{2024Yuqiarxiv:2401.06561v1,
 author = {Yuqi Zhang and Liang Ding and Lefei Zhang and Dacheng Tao},
 journal = {arxiv:2401.06561v1},
 title = {Intention Analysis Prompting Makes Large Language Models A Good
  Jailbreak Defender},
 url = {http://arxiv.org/abs/2401.06561v1},
 year = {2024}
}

@article{2023Zemingarxiv:2310.06387v1,
 author = {Zeming Wei and Yifei Wang and Yisen Wang},
 journal = {arxiv:2310.06387v1},
 title = {Jailbreak and Guard Aligned Language Models with Only Few In-Context
  Demonstrations},
 url = {http://arxiv.org/abs/2310.06387v1},
 year = {2023}
}

@misc{wang2023selfguard,
      title={Self-Guard: Empower the LLM to Safeguard Itself}, 
      author={Zezhong Wang and Fangkai Yang and Lu Wang and Pu Zhao and Hongru Wang and Liang Chen and Qingwei Lin and Kam-Fai Wong},
      year={2023},
      eprint={2310.15851},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Nicholasarxiv:2302.00871v3,
 author = {Nicholas Meade and Spandana Gella and Devamanyu Hazarika and Prakhar Gupta and Di Jin and Siva Reddy and Yang Liu and Dilek Hakkani-Tür},
 journal = {arxiv:2302.00871v3},
 title = {Using In-Context Learning to Improve Dialogue Safety},
 url = {http://arxiv.org/abs/2302.00871v3},
 year = {2023}
}

@article{Xie2023DefendingCA,
  title={Defending ChatGPT against jailbreak attack via self-reminders},
  author={Yueqi Xie and Jingwei Yi and Jiawei Shao and Justin Curl and Lingjuan Lyu and Qifeng Chen and Xing Xie and Fangzhao Wu},
  journal={Nature Machine Intelligence},
  year={2023},
  volume={5},
  pages={1486-1496},
  url={https://api.semanticscholar.org/CorpusID:266289038}
}

@misc{ji2023beavertails,
      title={BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset}, 
      author={Jiaming Ji and Mickel Liu and Juntao Dai and Xuehai Pan and Chi Zhang and Ce Bian and Chi Zhang and Ruiyang Sun and Yizhou Wang and Yaodong Yang},
      year={2023},
      eprint={2307.04657},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Nathanarxiv:2310.13595v2,
 author = {Nathan Lambert and Thomas Krendl Gilbert and Tom Zick},
 journal = {arxiv:2310.13595v2},
 title = {The History and Risks of Reinforcement Learning and Human Feedback},
 url = {http://arxiv.org/abs/2310.13595v2},
 year = {2023}
}

@misc{shi2023saferinstruct,
      title={Safer-Instruct: Aligning Language Models with Automated Preference Data}, 
      author={Taiwei Shi and Kai Chen and Jieyu Zhao},
      year={2023},
      eprint={2311.08685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Josefarxiv:2310.12773v1,
 author = {Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
 journal = {arxiv:2310.12773v1},
 title = {Safe RLHF: Safe Reinforcement Learning from Human Feedback},
 url = {http://arxiv.org/abs/2310.12773v1},
 year = {2023}
}

@misc{ge2023mart,
      title={MART: Improving LLM Safety with Multi-round Automatic Red-Teaming}, 
      author={Suyu Ge and Chunting Zhou and Rui Hou and Madian Khabsa and Yi-Chia Wang and Qifan Wang and Jiawei Han and Yuning Mao},
      year={2023},
      eprint={2311.07689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Traianarxiv:2310.10501v1,
 author = {Traian Rebedea and Razvan Dinu and Makesh Sreedhar and Christopher Parisien and Jonathan Cohen},
 journal = {arxiv:2310.10501v1},
 title = {NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications
  with Programmable Rails},
 url = {http://arxiv.org/abs/2310.10501v1},
 year = {2023}
}

@article{rai2024guardian,
  title={GUARDIAN: A Multi-Tiered Defense Architecture for Thwarting Prompt Injection Attacks on LLMs},
  author={Rai, Parijat and Sood, Saumil and Madisetti, Vijay K and Bahga, Arshdeep},
  journal={Journal of Software Engineering and Applications},
  volume={17},
  number={1},
  pages={43--68},
  year={2024},
  publisher={Scientific Research Publishing}
}

@article{2023Neelarxiv:2309.00614v2,
 author = {Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
 journal = {arxiv:2309.00614v2},
 title = {Baseline Defenses for Adversarial Attacks Against Aligned Language
  Models},
 url = {http://arxiv.org/abs/2309.00614v2},
 year = {2023}
}

@misc{pisano2023bergeron,
      title={Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework}, 
      author={Matthew Pisano and Peter Ly and Abraham Sanders and Bingsheng Yao and Dakuo Wang and Tomek Strzalkowski and Mei Si},
      year={2023},
      eprint={2312.00029},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{2023Bochengarxiv:2310.02417v1,
 author = {Bocheng Chen and Advait Paliwal and Qiben Yan},
 journal = {arxiv:2310.02417v1},
 title = {Jailbreaker in Jail: Moving Target Defense for Large Language Models},
 url = {http://arxiv.org/abs/2310.02417v1},
 year = {2023}
}

@article{2024Steffiarxiv:2401.05998v1,
 author = {Steffi Chern and Zhen Fan and Andy Liu},
 journal = {arxiv:2401.05998v1},
 title = {Combating Adversarial Attacks with Multi-Agent Debate},
 url = {http://arxiv.org/abs/2401.05998v1},
 year = {2024}
}

@article{2020Jingarxiv:2010.07079v3,
 author = {Jing Xu and Da Ju and Margaret Li and Y-Lan Boureau and Jason Weston and Emily Dinan},
 journal = {arxiv:2010.07079v3},
 title = {Recipes for Safety in Open-domain Chatbots},
 url = {http://arxiv.org/abs/2010.07079v3},
 year = {2020}
}

@misc{khalatbari2023learn,
      title={Learn What NOT to Learn: Towards Generative Safety in Chatbots}, 
      author={Leila Khalatbari and Yejin Bang and Dan Su and Willy Chung and Saeed Ghadimi and Hossein Sameti and Pascale Fung},
      year={2023},
      eprint={2304.11220},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2024Adibarxiv:2401.10862v1,
 author = {Adib Hasan and Ileana Rugina and Alex Wang},
 journal = {arxiv:2401.10862v1},
 title = {Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs
  Without Fine-Tuning},
 url = {http://arxiv.org/abs/2401.10862v1},
 year = {2024}
}

@article{2023Gabrielarxiv:2308.14132v3,
 author = {Gabriel Alon and Michael Kamfonas},
 journal = {arxiv:2308.14132v3},
 title = {Detecting Language Model Attacks with Perplexity},
 url = {http://arxiv.org/abs/2308.14132v3},
 year = {2023}
}

@article{2023Aounonarxiv:2309.02705v2,
 author = {Aounon Kumar and Chirag Agarwal and Suraj Srinivas and Aaron Jiaxun Li and Soheil Feizi and Himabindu Lakkaraju},
 journal = {arxiv:2309.02705v2},
 title = {Certifying LLM Safety against Adversarial Prompting},
 url = {http://arxiv.org/abs/2309.02705v2},
 year = {2023}
}

@misc{robey2023smoothllm,
      title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks}, 
      author={Alexander Robey and Eric Wong and Hamed Hassani and George J. Pappas},
      year={2023},
      eprint={2310.03684},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pi2024mllmprotector,
      title={MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance}, 
      author={Renjie Pi and Tianyang Han and Yueqi Xie and Rui Pan and Qing Lian and Hanze Dong and Jipeng Zhang and Tong Zhang},
      year={2024},
      eprint={2401.02906},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{2024Danielarxiv:2401.00991v1,
 author = {Daniel Wankit Yip and Aysan Esmradi and Chun Fai Chan},
 journal = {arxiv:2401.00991v1},
 title = {A Novel Evaluation Framework for Assessing Resilience Against Prompt
  Injection Attacks in Large Language Models},
 url = {http://arxiv.org/abs/2401.00991v1},
 year = {2024}
}

@misc{shu2024attackeval,
      title={AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models}, 
      author={Dong shu and Mingyu Jin and Suiyuan Zhu and Beichen Wang and Zihao Zhou and Chong Zhang and Yongfeng Zhang},
      year={2024},
      eprint={2401.09002},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Yimingarxiv:2307.06865v1,
 author = {Yiming Zhang and Daphne Ippolito},
 journal = {arxiv:2307.06865v1},
 title = {Prompts Should not be Seen as Secrets: Systematically Measuring Prompt
  Extraction Attack Success},
 url = {http://arxiv.org/abs/2307.06865v1},
 year = {2023}
}

@article{2023Erikarxiv:2305.08005v1,
 author = {Erik Derner and Kristina Batistič},
 journal = {arxiv:2305.08005v1},
 title = {Beyond the Safeguards: Exploring the Security Risks of ChatGPT},
 url = {http://arxiv.org/abs/2305.08005v1},
 year = {2023}
}

@article{2023Zhipingarxiv:2309.11653v1,
 author = {Zhiping Zhang and Michelle Jia and Hao-Ping and Lee and Bingsheng Yao and Sauvik Das and Ada Lerner and Dakuo Wang and Tianshi Li},
 journal = {arxiv:2309.11653v1},
 title = {"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure
  Risks and Benefits When Using LLM-Based Conversational Agents},
 url = {http://arxiv.org/abs/2309.11653v1},
 year = {2023}
}

@article{2023Chia-Chienarxiv:2311.14966v1,
 author = {Chia-Chien Hung and Wiem Ben Rim and Lindsay Frost and Lars Bruckner and Carolin Lawrence},
 journal = {arxiv:2311.14966v1},
 title = {Walking a Tightrope -- Evaluating Large Language Models in High-Risk
  Domains},
 url = {http://arxiv.org/abs/2311.14966v1},
 year = {2023}
}

@misc{xu2023scsafety,
      title={SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese}, 
      author={Liang Xu and Kangkang Zhao and Lei Zhu and Hang Xue},
      year={2023},
      eprint={2310.05818},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Loraarxiv:2306.11247v1,
 author = {Lora Aroyo and Alex S. Taylor and Mark Diaz and Christopher M. Homan and Alicia Parrish and Greg Serapio-Garcia and Vinodkumar Prabhakaran and Ding Wang},
 journal = {arxiv:2306.11247v1},
 title = {DICES Dataset: Diversity in Conversational AI Evaluation for Safety},
 url = {http://arxiv.org/abs/2306.11247v1},
 year = {2023}
}

@article{2023Jingweiarxiv:2312.14197v1,
 author = {Jingwei Yi and Yueqi Xie and Bin Zhu and Keegan Hines and Emre Kiciman and Guangzhong Sun and Xing Xie and Fangzhao Wu},
 journal = {arxiv:2312.14197v1},
 title = {Benchmarking and Defending Against Indirect Prompt Injection Attacks on
  Large Language Models},
 url = {http://arxiv.org/abs/2312.14197v1},
 year = {2023}
}

@article{2023Jiyanarxiv:2312.06632v1,
 author = {Jiyan He and Weitao Feng and Yaosen Min and Jingwei Yi and Kunsheng Tang and Shuai Li and Jie Zhang and Kejiang Chen and Wenbo Zhou and Xing Xie and Weiming Zhang and Nenghai Yu and Shuxin Zheng},
 journal = {arxiv:2312.06632v1},
 title = {Control Risk for Potential Misuse of Artificial Intelligence in Science},
 url = {http://arxiv.org/abs/2312.06632v1},
 year = {2023}
}

@misc{vidgen2023simplesafetytests,
      title={SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models}, 
      author={Bertie Vidgen and Hannah Rose Kirk and Rebecca Qian and Nino Scherrer and Anand Kannappan and Scott A. Hale and Paul Röttger},
      year={2023},
      eprint={2311.08370},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Samarxiv:2311.01011v1,
 author = {Sam Toyer and Olivia Watkins and Ethan Adrian Mendes and Justin Svegliato and Luke Bailey and Tiffany Wang and Isaac Ong and Karim Elmaaroufi and Pieter Abbeel and Trevor Darrell and Alan Ritter and Stuart Russell},
 journal = {arxiv:2311.01011v1},
 title = {Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game},
 url = {http://arxiv.org/abs/2311.01011v1},
 year = {2023}
}

@misc{zhang2023safetybench,
      title={SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions}, 
      author={Zhexin Zhang and Leqi Lei and Lindong Wu and Rui Sun and Yongkang Huang and Chong Long and Xiao Liu and Xuanyu Lei and Jie Tang and Minlie Huang},
      year={2023},
      eprint={2309.07045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023donotanswer,
      title={Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs}, 
      author={Yuxia Wang and Haonan Li and Xudong Han and Preslav Nakov and Timothy Baldwin},
      year={2023},
      eprint={2308.13387},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{röttger2023xstest,
      title={XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, 
      author={Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy},
      year={2023},
      eprint={2308.01263},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2023cvalues,
      title={CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility}, 
      author={Guohai Xu and Jiayi Liu and Ming Yan and Haotian Xu and Jinghui Si and Zhuoran Zhou and Peng Yi and Xing Gao and Jitao Sang and Rong Zhang and Ji Zhang and Chao Peng and Fei Huang and Jingren Zhou},
      year={2023},
      eprint={2307.09705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2023Huachuanarxiv:2307.08487v3,
 author = {Huachuan Qiu and Shuai Zhang and Anqi Li and Hongliang He and Zhenzhong Lan},
 journal = {arxiv:2307.08487v3},
 title = {Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output
  Robustness of Large Language Models},
 url = {http://arxiv.org/abs/2307.08487v3},
 year = {2023}
}

@article{2017Jiyangarxiv:1705.02101v2,
 author = {Jiyang Gao and Chen Sun and Zhenheng Yang and Ram Nevatia},
 journal = {arxiv:1705.02101v2},
 title = {TALL: Temporal Activity Localization via Language Query},
 url = {http://arxiv.org/abs/1705.02101v2},
 year = {2017}
}

@article{2023Yangarxiv:2312.17115v1,
 author = {Yang Xiao and Yi Cheng and Jinlan Fu and Jiashuo Wang and Wenjie Li and Pengfei Liu},
 journal = {arxiv:2312.17115v1},
 title = {How Far Are We from Believable AI Agents? A Framework for Evaluating the
  Believability of Human Behavior Simulation},
 url = {http://arxiv.org/abs/2312.17115v1},
 year = {2023}
}

@inproceedings{schulhoff-etal-2023-ignore,
  Title = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition},
  Author = {Sander V Schulhoff and Jeremy Pinto and Anaum Khan and Louis-FranÃois Bouchard and Chenglei Si and Jordan Lee Boyd-Graber and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher R Carnahan},
  Booktitle = {Empirical Methods in Natural Language Processing},
  Year = {2023},
  Location = {Singapore}
}


@misc{berglund2023reversal,
      title={The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"}, 
      author={Lukas Berglund and Meg Tong and Max Kaufmann and Mikita Balesni and Asa Cooper Stickland and Tomasz Korbak and Owain Evans},
      year={2023},
      eprint={2309.12288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{allenzhu2023physics,
      title={Physics of Language Models: Part 3.2, Knowledge Manipulation}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2023},
      eprint={2309.14402},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rubin-etal-2022-learning,
    title = "Learning To Retrieve Prompts for In-Context Learning",
    author = "Rubin, Ohad  and
      Herzig, Jonathan  and
      Berant, Jonathan",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.191",
    doi = "10.18653/v1/2022.naacl-main.191",
    pages = "2655--2671",
    abstract = "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.",
}
@inproceedings{maus2023black,
  title={Black box adversarial prompting for foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob R},
  booktitle={The Second Workshop on New Frontiers in Adversarial Machine Learning},
  year={2023}
}
@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}
@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  year={2023}
}
@article{pryzant2023automatic,
  title={Automatic prompt optimization with" gradient descent" and beam search},
  author={Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2305.03495},
  year={2023}
}
@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}
@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}
@article{mehrotra2023tree,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}
@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}


@book{2012,
  title = {Ensemble Machine Learning: Methods and Applications},
  ISBN = {9781441993267},
  url = {http://dx.doi.org/10.1007/978-1-4419-9326-7},
  DOI = {10.1007/978-1-4419-9326-7},
  publisher = {Springer New York},
  year = {2012}
}


@inproceedings{NEURIPS2022_9d560961,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24824--24837},
 publisher = {Curran Associates, Inc.},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{pan2020privacy,
  title={Privacy risks of general-purpose language models},
  author={Pan, Xudong and Zhang, Mi and Ji, Shouling and Yang, Min},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
  pages={1314--1331},
  year={2020},
  organization={IEEE}
}

@article{alawida2024unveiling,
  title={Unveiling the dark side of chatgpt: Exploring cyberattacks and enhancing user awareness},
  author={Alawida, Moatsum and Abu Shawar, Bayan and Abiodun, Oludare Isaac and Mehmood, Abid and Omolara, Abiodun Esther and Al Hwaitat, Ahmad K},
  journal={Information},
  volume={15},
  number={1},
  pages={27},
  year={2024},
  publisher={MDPI}
}

@article{subhash2023universal,
  title={Why do universal adversarial attacks work on large language models?: Geometry might be the answer},
  author={Subhash, Varshini and Bialas, Anna and Pan, Weiwei and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2309.00254},
  year={2023}
}

@inproceedings{pan2020privacy,
  title={Privacy risks of general-purpose language models},
  author={Pan, Xudong and Zhang, Mi and Ji, Shouling and Yang, Min},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
  pages={1314--1331},
  year={2020},
  organization={IEEE}
}

@misc{
anonymous2024tall,
title={Tall Tales at Different Scales: Evaluating Scaling Trends For Deception in Language Models},
author={Anonymous},
year={2024},
url={https://openreview.net/forum?id=YRXDl6I3j5}
}

@article{zhang2023s,
  title={It's a Fair Game, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents},
  author={Zhang, Zhiping and Jia, Michelle and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi and others},
  journal={arXiv preprint arXiv:2309.11653},
  year={2023}
}

@misc{das2024security,
      title={Security and Privacy Challenges of Large Language Models: A Survey}, 
      author={Badhan Chandra Das and M. Hadi Amini and Yanzhao Wu},
      year={2024},
      eprint={2402.00888},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}